{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944a3d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# generate x_train and y_train from train data\n",
    "# note that for instances containing multiple authors, generate one x_train for each author(label)\n",
    "def convert_train_data(data):\n",
    "    x_array=[]   #[{\"venue\":value},{\"keywords\":value},{\"year\":value},{\"coauthors\":value}]\n",
    "    y_array=[]   #[author name]\n",
    "    for id,instance in data.items(): #for each instance\n",
    "        for i in range(len(instance[\"author\"])):\n",
    "            instance_dict=get_instance_dict(instance,instance[\"author\"][i]) \n",
    "            x_array.append(instance_dict)\n",
    "            y_array.append(instance[\"author\"][i])\n",
    "    return x_array,y_array   #,targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# internal method for convert_train_data\n",
    "# note that the coauthor feature is not used (commented out)\n",
    "def get_instance_dict(instance,author): #exclude this author from coauthor list\n",
    "    instance_dict={}\n",
    "    for name, value in instance.items():\n",
    "        # for features in train/test data\n",
    "        if name==\"year\" or name==\"venue\":  \n",
    "            if value ==\"\":  #avoid empty value\n",
    "                instance_dict[name]= -1   #TODO too large may cause normalization problem\n",
    "            else:\n",
    "                instance_dict[name]=value\n",
    "        if name==\"keywords\":\n",
    "            instance_dict[name]=[str(x) for x in value]   \n",
    "\n",
    "    return instance_dict\n",
    "                \n",
    "\n",
    "def convert_test_data(data):\n",
    "    x_array=[]   #[{feature name:value},{feature name:value}]\n",
    "    coauthors=[]  # [[label]]\n",
    "    targets=[]\n",
    "    for id,instance in data.items(): #for each instance\n",
    "        instance_dict={}\n",
    "        for name, value in instance.items():\n",
    "            # for features in train/test data\n",
    "            if name==\"year\" or name==\"venue\":  \n",
    "                if value ==\"\": \n",
    "                    instance_dict[name]= -1   \n",
    "                else:\n",
    "                    instance_dict[name]=value\n",
    "            if name==\"keywords\":\n",
    "                instance_dict[name]=[str(x) for x in value]  \n",
    "\n",
    "            # coauthor(s) for test data\n",
    "            if name==\"coauthor\":    \n",
    "                coauthors.append(value)\n",
    "            # target for test data\n",
    "            if name==\"target\":\n",
    "                targets.append(value)\n",
    "        x_array.append(instance_dict)\n",
    "        \n",
    "    return x_array,coauthors,targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc9d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()  \n",
    "\n",
    "# train data\n",
    "train_file_path=\"train.json\"\n",
    "train_data = json.load(open(train_file_path, \"r\"))  #dict\n",
    "\n",
    "x_train,y_train= convert_train_data(train_data) \n",
    "#convert to [500-keyword,venue,year] vector (dimension= 502)\n",
    "x_train= vec.fit_transform(x_train).toarray()  \n",
    "\n",
    "\n",
    "#test data\n",
    "test_file_path=\"test.json\"\n",
    "test_data = json.load(open(test_file_path, \"r\"))  #dict\n",
    "\n",
    "x_test,coauthors,targets= convert_test_data(test_data)  \n",
    "x_test= vec.transform(x_test).toarray() \n",
    "\n",
    "# Since SVMs incorporate a penalty term for the weights (proportional to ‖𝐰‖2 ), \n",
    "# standardise features so that each feature has zero mean/unit variance.\n",
    "scaler = StandardScaler(with_mean=False)# with_mean-center data before scaling-->not work on sparse matrix\n",
    "x_train = scaler.fit_transform(x_train) \n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split train data into train and dev data\n",
    "# # grid search for SVC to find optimal set of parameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_dev, y_tr, y_dev = train_test_split(x_train, y_train, test_size=0.2, random_state=52)\n",
    "\n",
    "param_grid = {'kernel':['linear','rbf'],  \n",
    "              'gamma':['scale', 'auto'],  \n",
    "              'C':[0.5,1.0,1.5]\n",
    "             }\n",
    "\n",
    "svc = SVC()\n",
    "gs = GridSearchCV(estimator=svc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy', \n",
    "                  n_jobs= 2,   \n",
    "                  verbose=3,\n",
    "                  cv=2)\n",
    "gs.fit(x_tr, y_tr)\n",
    "best_params = gs.best_params_\n",
    "print(\"Best parameters for grid search are \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c64388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using the optimal parameter set\n",
    "svc = SVC(kernel=\"linear\",C=0.6, class_weight='balanced', probability=True) #linear kernel\n",
    "model = svc.fit(x_train, y_train)  #x_train_real\n",
    "prob_distribution= model.predict_proba(x_test) #predicted prob distribution over each author\n",
    "\n",
    "import csv\n",
    "prediction_file=\"test_submission.csv\"\n",
    "f = open(prediction_file, 'w')\n",
    "writer = csv.writer(f)\n",
    "\n",
    "prob_list=[]\n",
    "for i in range(len(x_test)):\n",
    "    author_index=targets[i]\n",
    "    prob_list.append([i,prob_distribution[i][author_index]])\n",
    "# print(prob_list)\n",
    "\n",
    "writer.writerow([\"Id\",\"Predicted\"])\n",
    "for line in prob_list:\n",
    "    writer.writerow(line)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fff4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CalibratedClassifierCV with LinearSVC\n",
    "\n",
    "import csv\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC   \n",
    "\n",
    "# Prefer dual=False when n_samples > n_features.\n",
    "svm = LinearSVC(penalty=\"l1\",dual=False) #linear kernel   # penalty{‘l1’, ‘l2’}, default=’l2’\n",
    "clf = CalibratedClassifierCV(svm) \n",
    "\n",
    "clf.fit(x_train, y_train)  #x_train_real\n",
    "prob_distribution= clf.predict_proba(x_test) #predicted prob distribution over each author\n",
    "\n",
    "prediction_file=\"test_submission7.csv\"\n",
    "f = open(prediction_file, 'w')\n",
    "writer = csv.writer(f)\n",
    "\n",
    "prob_list=[]\n",
    "for i in range(len(x_test)):\n",
    "    author_index=targets[i]\n",
    "    prob_list.append([i,prob_distribution[i][author_index]])\n",
    "\n",
    "writer.writerow([\"Id\",\"Predicted\"])\n",
    "for line in prob_list:\n",
    "    writer.writerow(line)\n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
